**读前须知**：其中美团主要是在美团官方技术文档中找的，滴滴是通过官方发布的论文，由于本人能力不足，其中很多东西不怎么懂，基本靠AI帮助进行总结，因为美团部分大部分手打，可能会有文字错误，请谅解。美团能找到的资料很少，只有官方的几篇，还发现了官方的数学公式有笔误（hahaha），在文中已更正；在文中已经附上链接；滴滴派单那块网络上可以找到相关的论文，我也使用AI对文章进行了一定的总结，滴滴派单相关的论文也放在资料中了，需要进一步了解可以查看。

## 1. 美团

在2018年11月，美团发布的“美团即时物流的分布式系统架构设计”中，展示的系统的架构如下图所示

![美团架构](.\pic_source\美团架构.png)

​										图1. 美团系统架构

可以看到，美团在2018年的系统架构中已经使用深度学习、运筹优化、NLP等算法进行上层实现的支撑，例如网络规划、路径规划、交互点规划以及运力供应调控等系统功能。

在2018年7月的关于“美团配送系统架构演进实践”中，提到了算法数据平台，用大数据相关技术存储数据，为机器学习和深度学习算法模型在配送各个业务线落地提供支撑。

由此看来，美团的配送业务在算法方面大规模的使用了深度学习、机器学习的相关算法，对美团配送整个业务流程进行规划部署。

### 1.1 O2O即时配送智能调度系统

在美团2017年美团技术团队发布的“即时配送的订单分配策略：从建模和优化中”，讲述了O2O即时配送调度系统的运行流程，步骤如下：

> 系统首先通过**优化设定配送费**以及预**计送达时间**来调整订单结构；

> 在接收订单之后，考虑骑手位置、在途订单情况、骑手能力、商家出餐、交付难度、天气、地理路况、未来单量等因素，在正确的时间将订单分配给最合适的骑手，并在骑手执行过程中随时预判订单超时情况并动态触发改派操作，实现订单和骑手的动态最优匹配；

> 同时，系统派单后，为骑手提示该商家的预计出餐时间和合理的配送线路，并通过语音方式和骑手实现高效交互；

> 在骑手送完订单后，系统根据订单需求预测和运力分布情况，告知骑手不同商圈的运力需求情况，实现闲时的运力调度。

最后提到：通过上述技术和模式的引入，持续改善了用户体验和配送成本：订单的平均配送时长从2015年的41分钟，下降到32分钟，进一步缩短至28分钟，另一方面，在骑手薪资稳步提升的前提下，单均配送成本也有了20%以上的缩减。

### 1.2 订单调度（2017）

美团基于大数据平台，根据订单的配送需求、地理环境以及每名骑手的个性化特点，实现订单与骑手的高效动态最优匹配，从而为每个用户和商家提供最佳的配送服务，并降低配送成本。

![美团模型](.\pic_source\美团模型.png)

​									图2. 美团算法模型

决策优化类数学模型一般包括三个要素：**决策变量**、**优化目标**、**约束条件**。2017年10月美团发布的文章中，首先定义了若干符号，订单分配问题中，定义如下：

$$
n:\text订单数量\\
m:\text骑手数量\\
r_i:\text订单i的下单时刻\\
d_i:\text订单i的期望送达时间\\
p_i:\text订单i的备货消耗时长\\
v_j:\text骑手j的骑行速度\\
u_i:\text订单i的交付消耗时长\\
pos_j:\text骑手j的当前位置\\
(i,B):\text订单i的取货任务\\
(i,C):\text订单i的送货任务\\
\Omega:\text所有任务集合\\
pos(i,B):\text订单i的取货位置\\
pos(i,C):\text订单i的送货位置\\
l_{pos1,pos2}:\text pos1到pos2的导航距离
$$
在即时配送调度场景下，决策变量包括各个订单需要分配的骑手，以及骑手的建议行驶路线。
$$
\Omega_j:\text骑手j所分配的任务集合\\
seq_j:\text骑手j所分配任务 执行顺序\\
fr_{(i,B)}:\text订单i的到达商户时刻\\
f_{(i,B)}:\text订单i的取货时刻\\
fr_{(i,C)}:\text订单i的到达用户时刻\\
f_{(i,C)}:\text订单i的送达时刻
$$
即时配送订单分配问题的优化目标一般包括希望用户的单均配送时长尽量短、骑手付出的劳动尽量少、超时率尽量低，等等。一般可表达为：
$$
\text{目标1：最小化超时率}\\
\text{min}g_1(\Omega) = \frac{\sum_{i=1}^n 1(f_{(i,C)}\geq d_i)}{n}\\
\text目标2：最小单均行驶距离\\
\text{min}g_2(\Omega) = \frac{\sum_{j=1}^m\sum_{k1,k2\in{seq_j}} l_{k1,k2})}{n}\\
\text目标1：最小化单均消耗时间\\
\text{min}g_3(\Omega) = \frac{\sum_{j=1}^m\underset{(i,x)\in{seq_j}}{max}T_{(i,x)}}{n}\\
$$

为了更好的解决实际场景中的订饭分配问题，设置了一些约束条件如下：
$$
\text{约束1：一个订单的送货要在取货后进行}\\
f_{i,B}+\frac{l_{pos(i,B),pos(i,C)}}{\underset{j}{\text{max}}\,v_j} \leq fr_{(i,C)}\,+\,u_i\,=\,f_{i,C}\\
\text{约束2：订单取货需要备货完成和骑手到达都具备后才能进行}\\
f_{i,B}\,\geq\,r_i+p_i\;\forall{i}\,=\,1,2,\cdot\cdot\cdot,n\\
\text{约束3：同一个骑手所分配的多项任务的完成时间限制}\\
f_{(i1,x1)}\geq f_{i2,x2} + \frac{l_{pos(i1,x1),pos(i2,x2)}}{v_j}\\
\forall(i1,x1),(i2,x2)\in\Omega_j,\forall j=1,2,\cdot\cdot\cdot,m\\
\text{约束4：一个订单的取送，要由同一名骑手完成}\\
\forall(i,B)(i,C)\in\Omega,\\
\exists j\text{满足}\, (i,B)(i,C)\in\Omega_j
$$
还考虑到了一些特殊情况的配送，部分订单只能由具备某些特点的骑手来配送（例如火锅订单只能交给携带专门装备的骑手等）、载具的容量限制等。

<img src="https://awps-assets.meituan.net/mit-x/blog-images-bundle-2017/d2ce29cb.png" alt="7"  />

#### 优化过程

目标不是单次决策的最优，而是通过策略在一段时间内的应用实现累积收益最大化。由于未来订单信息的不确定性，建模变得更加复杂。运筹优化中的**马尔可夫决策过程**（MDP）可以用于解决这种不确定环境下的序贯决策问题。

![8](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2017/ba187fda.png)

信息化水平较低时工业运筹优化项目失败的原因之一是**缺乏完备的数据采集工具**，导致模型优化结果不准确。在即时配送订单分配中，有两类数据：一类是直接通过业务系统获取的数据，如订单和骑手状态；另一类是需要预测的数据，如商户出餐时间和用户交付时间。准确估计这些时间可以提升配送效率。由于许多影响因素无法直接采集，利用**机器学习**结合**历史数据**和**外部因素**（如天气）来预测出餐和交付时间，从而提高模型的准确性。

![9](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2017/12962ede.png)

#### 完备采集数据后的优化效果

进一步，我们建立了调度模型的自学习机制，借鉴多变量控制理论的思想，不断根据预估偏差调整预估模型中的相关参数。通过以上工作，我们通过调度模型来预估骑手的配送行为（取餐时间和送达时间），平均偏差小于4分钟，10分钟置信度达到90%以上，有效地提升了派单效果和用户满意度。

#### 配送调度问题的建模和优化

目标是构建一个与实际业务吻合的解空间，并在其中找到最优策略。由于配送调度是NP-Hard问题，解空间巨大且难以找到最优解，因此需要高效的优化算法，并且必须在2-3秒内做出决策。

为解决这个难题，采用了两个关键思路：

1. **问题特征分析**：通过深入分析问题特点，设计性能优越的算法。这涉及运用如单纯形法、梯度下降、遗传算法、模拟退火和动态规划等基础算法。
2. **分层优化**：将问题分解为骑手路径优化和订单分配优化。路径优化确定骑手最佳配送路线，分配优化则将订单分配给骑手以优化指标。两者迭代进行，最终获得满意的解。

<img src="https://awps-assets.meituan.net/mit-x/blog-images-bundle-2017/77fda2f3.png" alt="10" style="zoom: 50%;" />

#### 跨学科结合

1. **方法分类**：
   - **图论方法**：将订单分配转换为二分图匹配问题，利用匈牙利算法或KM算法。通过打包订单简化问题，优点是实现简单且计算快速，但可能损失部分满意解。
   - **个性化算法**：直接优化订单分配方案，虽然更难实现，但不损失满意解的可能性。
2. **综合策略**：
   - 结合领域知识、优化算法、机器学习和图论算法，设计骑手路径和订单分配优化算法。
   - 引入强化学习，结合离线学习和在线优化，通过策略迭代不断改进。

最终，通过这些方法，优化效果提升了50%以上，同时减少了耗时。在大量的实际数据集上进行评估验证，99%以上的情况下，骑手路径优化算法能够在30ms内给出最优解。为了有效降低算法运行时间，我们对优化算法进行并行化，并利用并行计算集群进行快速处理。一个区域的调度计算会在数百台计算机上同步执行，在2~3秒内返回满意结果，每天的路径规划次数超过50亿次。

即时配送过程中，由于线下突发因素（如商家出餐慢、联系不上用户、交通管制等），订单分配可能变得不合理，导致订单超时和骑手等待时间增加，影响配送效率和用户体验。

目前，解决这些问题主要依赖人工处理。配送站长或调度员通过信息系统查看骑手位置、订单状态等信息，并接听骑手的改派请求，分析哪些订单需要改派以及改派给谁，然后执行改派操作。

<img src="https://awps-assets.meituan.net/mit-x/blog-images-bundle-2017/7612e7e6.png" alt="11" style="zoom: 50%;" />

针对即时配送的强不确定性特点，美团技术团队提出了两点创新：一是**延迟调度策略**，即在某些场景订单可以不被指派出去，在不影响订单超时的情况下，延迟做出决策；二是系统**自动改派策略**，即订单即便已经派给了骑手，后台的智能算法仍然会实时评估各个骑手的位置、订单情况，并帮助骑手进行分析，判断是否存在超时风险。如果存在，则系统会评估是否有更优的骑手来配送。

延迟调度的好处一方面是在动态多变的不确定环境下，寻求最佳的订单指派时机，以提高效率；另一方面是在订单高峰时段存在大量堆积时，减轻骑手的配送压力。

有了这两项策略，订单的调度过程更加立体、全面，覆盖了订单履行过程全生命周期中的主要优化环节，实现订单和骑手的动态最优化匹配。

### 1.3 模型平台（2020）

在美团2020年发布的技术文档中，对配送调度场景再一次进行了描述，提出配送调度场景可以用数学语言描述，不仅是业务问题，更是标准组合优化问题，是一个马尔可夫决策过程，在文章末尾有关于马尔可夫决策过程的介绍。下图是美团技术团队对于该场景的数学描述。

![调度问题的数学描述](https://p0.meituan.net/travelcube/30e6df539239ed15cb77f5e67fec4d69454401.png)

可以看到该文章关于订单智能调度的描述与2017年的说法基本类似。

对于时间窗维度的描述，也就是考虑当前派送面对后面其他订单的影响，也考虑到了长周期的优化，相对于17年的那篇文章中，图示有所变化，下面是关于时间上派送问题的简化描述，这里更加精炼。

![问题简化分析](https://p0.meituan.net/travelcube/5ad071684bcf352e1500ee4b79769643286855.png)

关于NP-hard问题的相关描述在文章末尾有所介绍。



综合2017年和2020年的两篇文章，可以看出美团的技术团队主要有三个挑战：

性能问题：对大量数据中进行处理得到一个决策，需要进行非常庞大的运算；

动态性：很多场景下需要对未来一段时间发生的问题进行预估，决策空间太大；

配送业务随机因素过多，从商家，配送员，顾客到天气，等等因素，随机场景过多，运算量也会大幅增加。

### 1.4 参考文档

 **[美团即时物流的分布式系统架构设计](https://tech.meituan.com/2018/11/22/instant-logistics-distributed-system-architecture.html)** 
 **[美团配送系统架构演进实践](https://tech.meituan.com/2018/07/26/peisong-sys-arch-evolution.html)** 
 **[美团智能配送系统的运筹优化实战](https://tech.meituan.com/2020/02/20/meituan-delivery-operations-research.html)** 
 **[即时配送的订单分配策略：从建模和优化](https://tech.meituan.com/2017/10/11/o2o-intelligent-distribution.html)** 



## 2. 滴滴

### 2.1 KDD2018

#### 2.1.1 方法简述

 提出融合强化学习和组合优化的框架。
 使用马尔可夫决策过程（MDP）进行建模，并通过强化学习求解。
 将司乘匹配建模为组合优化问题，以获得全局最优解。

#### 2.1.2 算法与模型

##### 2.1.2.1 **马尔可夫决策过程 (MDP)**

​	通过将派单过程建模为MDP，定义司机为智能体，状态（s）包含司机的时空信息，动作（a）包括完成订单或空闲操作。

##### 2.1.2.2 **强化学习**

​	使用强化学习求解MDP，以最大化长期收益。
​	价值函数（Value Function）代表司机在特定状态下的预期收益。

##### 2.1.2.3 **组合优化**:

​	针对司机与乘客的多对多匹配，建模为组合优化问题。
​	使用Kuhn-Munkres (KM) 算法进行二分图匹配，优化派单决策。

#### 2.1.3 实现步骤

  ##### 离线部分

   **数据收集**:

​	收集历史订单数据，转化为强化学习中的四元组形式。

   **价值函数计算**:

​	使用动态规划求解每个时空状态下的价值函数，并以查找表形式保存。

  ##### 在线部分

   **实时数据处理**:

​	收集待分配司机和订单列表。

   **动作价值计算**:

​	计算每个司乘匹配的动作价值数，并建立二分图。

   **最优匹配求解**

​	将匹配权值嵌入KM算法，考虑接驾距离、服务分等因素，求解最优匹配。

  ##### 迭代更新

​	动态更新:

​		根据新数据离线更新价值函数，并使用更新后的价值函数指导派单。

#### 2.1.4 效果

​	实验与在线A/B测试结果显示，算法在确保乘客体验的同时显著提升了司机的收入。

​	该算法已在二十多个核心城市成功部署，满足了广泛用户的出行需求。

​	新算法不同于传统策略，能够考虑每次派单对未来司机分布的影响，面向长期收益。

### 2.2 KDD2019
#### 2.2.1  算法和模型
##### 2.2.1.1 半马尔可夫决策过程（SMDP）

- **定义**：
  
  SMDP是一种决策框架，适用于时间和状态变化的环境。它允许在多个时间尺度上进行决策，适合动态的订单调度问题。
- **与标准MDP的区别**：
  
  标准马尔可夫决策过程（MDP）假设每个决策都是在固定的时间间隔内进行的，而SMDP允许在不同的时间间隔内进行决策，适应性更强。

##### 2.2.1.2 小脑价值网络（CVNet）

- **结构**：
  
  CVNet是一个深度学习模型，设计用于评估在复杂环境中的决策价值。其结构包括多个层次，能够处理高维状态空间。
- **Lipschitz正则化**：
  
  在政策评估中引入Lipschitz正则化，以确保模型稳定性，防止过拟合。
- **上下文随机化技术**：
  
  通过上下文随机化，模型能够更好地学习特征，增强对不同环境的适应性。

##### 2.2.1.3 任务规划方法

- **实时匹配**：
  
  通过解决组合优化问题，实时计算司机与订单之间的匹配，提高调度效率。
- **迁移学习**：
  
  实施了三种网络模型用于跨城市的知识迁移：
  - **Fine-tuning**：在源城市训练后，将权重初始化到目标城市网络中，继续在新数据集上训练。
  - **Progressive Network**：确保在不同城市间有效转移知识，关注适应性输入（如上下文特征和时间）。
  - **相关特征渐进转移（CFPT）**：最大化从适应性输入中转移知识，同时专注于特定任务的非适应性部分（如GPS位置）。

#### 2. 实现方式

- **状态表示层**：
  
  使用分布式状态表示层来处理复杂的输入数据，确保模型的灵活性和适应性。
- **动态编程**：
  
  在离线学习阶段，利用动态编程方法进行时序差分更新，优化决策过程。
- **组合优化**：
  
  结合组合优化技术，提升调度模型的整体性能。

#### 3. 效果评估

- **性能提升**：
  
  通过大型在线A/B测试，CVNet在司机收入和用户体验方面的表现显著优于传统方法。
- **适应性增强**：
  
  应用迁移学习技术，增强模型在不同城市间的适应能力，提高了整体调度效率。
- **空间与时间动态**：
  
  强调了空间和时间动态对优化调度决策的重要性，进一步提升了调度的成功率。



### 2.3 KDD2023 ProbTTE
本文是预测从出发地到目的地的预期行程持续时间。非派单算法模型，作为补充放在这里。

#### ProbTTE框架

- **核心**：将单标签回归任务转换为多类分类问题，估计隐式旅行时间分布。

- 方法：

  1. **分布旅行时间编码（DTTE）**：将真实行程时间编码为one-hot向量，解决长尾分布问题。
  2. **自适应局部标签平滑（ALLS）**：缓解模型对目标类过于自信的问题。
  3. **路线级先验正则化**：利用滴滴的大规模历史旅行数据，提供路线长尾先验。

#### 结论

- ProbTTE框架通过明确量化出行时间分布并将其纳入现有的TTE服务，提高了出行时间预测的准确性，有利于各种下游网约车任务。
- 该框架与模型无关，这意味着它可以轻松地与其他TTE模型集成并扩展以支持一般回归问题。

## 3. 相关介绍

### 3.1 **马尔可夫决策过程**

马尔可夫决策过程（MDP）是一种数学模型，用于描述决策过程中的随机性和动态性。MDP由以下几个元素组成：

1. **状态集 (S*S*)**：系统可能的所有状态的集合。
2. **动作集 (A*A*)**：可用的所有动作集合。
3. **状态转移概率 (P*P*)**：从一个状态转移到另一个状态的概率，通常表示为 P(s′∣s,a)*P*(*s*′∣*s*,*a*)，即在状态 s*s* 下采取动作 a*a* 后转移到状态 s′*s*′ 的概率。
4. **奖励函数 (R*R*)**：执行某个动作后获得的即时奖励，表示为 R(s,a)*R*(*s*,*a*)。
5. **折扣因子 (γ*γ*)**：用于权衡当前和未来奖励的相对重要性，范围在 [0, 1] 之间。

MDP用于建模和解决涉及决策和不确定性的优化问题，广泛应用于强化学习、机器人控制、经济学等领域。



### 3.2 **NP-hard问题**

NP-hard问题是计算复杂性理论中的一类问题。它们具有以下特点：

1. **难度**：如果一个问题是NP-hard，那么解决它至少和解决NP问题一样困难。NP问题是指可以在多项式时间内验证解的正确性的问题。
2. **求解时间**：目前没有已知的多项式时间算法可以解决所有NP-hard问题。
3. **归约**：如果一个NP-hard问题能在多项式时间内解决，那么所有NP问题都能在多项式时间内解决。

常见的NP-hard问题包括旅行商问题、背包问题和图着色问题。这些问题在计算机科学、运筹学等领域中具有重要意义
### 3.3 **强化学习**

强化学习是一种机器学习方法，通过与环境的交互来学习策略，以最大化累积奖励。其基本概念包括：

1. **代理（Agent）**：执行动作并学习策略的实体。

2. **环境（Environment）**：代理与之交互的外部系统。

3. **状态（State）**：环境在某一时刻的具体情况。

4. **动作（Action）**：代理在某一状态下可以执行的操作。

5. **奖励（Reward）**：代理执行动作后从环境获得的反馈信号。

6. **策略（Policy）**：代理在每个状态选择动作的规则或函数。

7. **价值函数（Value Function）**：评估每个状态或状态-动作对的长期收益。

强化学习的目标是找到一个策略，使得在长期内累积的奖励最大化。常见算法包括Q学习、深度Q网络（DQN）、策略梯度方法和近端策略优化（PPO）等。强化学习被广泛应用于机器人控制、游戏AI、自动驾驶等领域。

### 3.4 **组合优化**

组合优化是优化问题的一个分支，涉及在有限或可数无限的解空间中寻找最优解。其特点包括：

1. 离散性：解空间通常是离散的，比如图、集合或序列。
2. 复杂性：许多组合优化问题是NP-hard的，求解难度高。
3. 应用广泛：常见于物流、网络设计、生产调度等领域。

**典型问题**

1. 旅行商问题（TSP）：寻找最短路径经过所有城市。
2. 背包问题：在容量限制下选择物品以最大化价值。
3. 图着色问题：用最少颜色对图进行着色，使相邻节点不同色。

**求解方法**

1. 精确算法：如分支定界、动态规划。
2. 近似算法：如贪心算法、启发式算法。
3. 元启发式算法：如遗传算法、模拟退火、粒子群优化。

### 3.5 半马尔可夫决策过程

半马尔可夫决策过程（SMDP）是马尔可夫决策过程（MDP）的扩展，允许动作持续一段时间而不是瞬时完成。SMDP用于建模那些决策和状态转移不在固定时间步长上发生的过程。

**关键要素**

1. **状态集 (S*S*)**：系统可能的所有状态。
2. **动作集 (A*A*)**：可用的所有动作。
3. **状态转移概率 (P*P*)**：从状态 s*s* 在动作 a*a* 后经过时间 t*t* 转移到状态 s′*s*′ 的概率。
4. **奖励函数 (R*R*)**：在状态 s*s* 下执行动作 a*a* 并经过时间 t*t* 获得的奖励。
5. **时间模型**：考虑动作执行的时间长度。

**应用**

SMDP适用于那些需要考虑动作执行时间的领域，如：

- 机器人路径规划
- 电信网络中的资源分配
- 制造系统中的生产调度

SMDP提供了一个框架，可以在不同时长的决策过程中优化策略，以最大化长期收益。

### 3.6 **小脑价值网络**

小脑价值网络（CVNet）是一种基于神经网络的模型，用于模拟和理解小脑在决策和运动控制中的作用。它通常涉及以下方面：

1. **价值评估**：CVNet通过学习评估不同动作或状态的价值，帮助优化决策过程。
2. **运动控制**：模拟小脑在精细运动控制中的功能，增强动作的协调性和准确性。
3. **学习机制**：使用强化学习等方法，训练网络以适应动态环境和任务要求。

CVNet的研究有助于揭示小脑在认知和运动任务中的功能，并在机器人学和神经科学领域中得到应用。

